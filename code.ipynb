{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3df2b4",
   "metadata": {},
   "source": [
    "1. Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5d5c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes, make_regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0280831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   T_degC   Salnty  O2ml_L  Depthm  Bottom_D  Wind_Spd  Dry_T  Wet_T  \\\n",
      "0  16.830  33.8510   5.560      65    1337.0      14.0   16.5   15.5   \n",
      "1   9.262  33.8481   2.729     140    1202.0       5.0   15.0   13.0   \n",
      "2  15.390  33.4260   5.990      30    3871.0      10.0   18.8   17.6   \n",
      "3  14.540  32.9470   5.840      42    4018.0      14.0   16.9   16.1   \n",
      "4   7.410  34.1810   1.000     300    4058.0      21.0   16.3   14.9   \n",
      "\n",
      "                       Wea      Cloud_Typ                  Cloud_Amt  \\\n",
      "0                      NaN            NaN                        NaN   \n",
      "1            Partly Cloudy        Stratus  1/10 or less but not zero   \n",
      "2  Continuous blowing snow        Stratus                      10/10   \n",
      "3  Continuous blowing snow  Stratocumulus                      10/10   \n",
      "4            Partly Cloudy  Stratocumulus               7/10 to 8/10   \n",
      "\n",
      "     Visibility  \n",
      "0           NaN  \n",
      "1  10km to 20km  \n",
      "2  10km to 20km  \n",
      "3   4km to 10km  \n",
      "4   4km to 10km  \n",
      "   T_degC   Salnty  O2ml_L  Depthm  Bottom_D  Wind_Spd  Dry_T  Wet_T\n",
      "0  16.830  33.8510   5.560      65    1337.0      14.0   16.5   15.5\n",
      "1   9.262  33.8481   2.729     140    1202.0       5.0   15.0   13.0\n",
      "2  15.390  33.4260   5.990      30    3871.0      10.0   18.8   17.6\n",
      "3  14.540  32.9470   5.840      42    4018.0      14.0   16.9   16.1\n",
      "4   7.410  34.1810   1.000     300    4058.0      21.0   16.3   14.9\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "url = \"https://raw.githubusercontent.com/rhodes-byu/stat-486/main/data/OceanicFisheries/ocean_data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(df.head())\n",
    "\n",
    "# Drop columns that are not needed\n",
    "df = df.drop(columns=['Wea', 'Cloud_Typ', 'Cloud_Amt', 'Visibility'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1c6225e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 7) (1777, 7)\n"
     ]
    }
   ],
   "source": [
    "# Create training data and testing data\n",
    "X = df.drop(columns=['T_degC'])\n",
    "y = df['T_degC']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    random_state=307, \n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89749d31",
   "metadata": {},
   "source": [
    "2. Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16f3f3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 1.5350956362420822\n",
      "Test MSE: 1.7543308610948438\n",
      "Variance of y_test: 14.8162872136925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.11840556515896747)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a) build a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# b) fit pipline to training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# c) report on the training MSE\n",
    "y_train_pred = pipe.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "\n",
    "# d) report on the test MSE\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "\n",
    "# e) how does the test MSE compare to the variance of ytest? What does this say about the predictability of your model?\n",
    "y_test_variance = np.var(y_test)\n",
    "print(f\"Variance of y_test: {y_test_variance}\")\n",
    "test_mse / y_test_variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae643c",
   "metadata": {},
   "source": [
    "3. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f930f7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y-intercept: 11.607865265528682\n"
     ]
    }
   ],
   "source": [
    "# a) use pipeline to order features by magnitude of coefficients\n",
    "betas = pipe.named_steps['model'].coef_\n",
    "feature_names = pipe.named_steps['poly'].get_feature_names_out(X_train.columns)\n",
    "\n",
    "# df of coefficients and feature names\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': betas\n",
    "})\n",
    "\n",
    "# order by magnitude of coefficients\n",
    "coef_df['abs_coef'] = coef_df['coefficient'].abs()\n",
    "coef_df_sorted = coef_df.sort_values('abs_coef', ascending=False)\n",
    "\n",
    "coef_df_sorted.head(10)\n",
    "\n",
    "# b top 3 largest positive\n",
    "top_positive = coef_df_sorted.sort_values('coefficient', ascending=False).head(3)\n",
    "top_positive\n",
    "\n",
    "# c top 3 largest negative\n",
    "top_negative = coef_df_sorted.sort_values('coefficient').head(3)\n",
    "top_negative\n",
    "\n",
    "# d y-intercept\n",
    "intercept = pipe.named_steps['model'].intercept_\n",
    "print(f\"y-intercept: {intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c608030",
   "metadata": {},
   "source": [
    "4. Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f329fdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'impute__strategy': 'median',\n",
       "  'model__n_neighbors': 10,\n",
       "  'model__weights': 'distance',\n",
       "  'poly__degree': 3},\n",
       " np.float64(1.1942865460094283))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a) fit a KNN model using the same pipline\n",
    "pipe_knn = Pipeline([\n",
    "  ('impute', SimpleImputer()),\n",
    "  ('poly', PolynomialFeatures(include_bias=False)),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('model', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# b) use grid search with 10-fold cross validation to find the best hyperparameters\n",
    "param_grid = {\n",
    "    'impute__strategy': ['mean', 'median'],\n",
    "    'poly__degree': [1, 2, 3],\n",
    "    'model__n_neighbors': list(range(5, 101, 5)),\n",
    "    'model__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe_knn,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "# c) report the best hyperparameters by MSE\n",
    "gs.fit(X_train, y_train)\n",
    "best_params = gs.best_params_\n",
    "best_mse = -gs.best_score_\n",
    "\n",
    "best_params, best_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a8b508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Test MSE: 1.2398437245052845\n"
     ]
    }
   ],
   "source": [
    "# e) use optimized pipline to predit on test data\n",
    "best_model = gs.best_estimator_\n",
    "y_test_pred_knn = best_model.predict(X_test)\n",
    "test_mse_knn = mean_squared_error(y_test, y_test_pred_knn)\n",
    "print(f\"KNN Test MSE: {test_mse_knn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c1a89",
   "metadata": {},
   "source": [
    "5. Randomized Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aae44c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Test MSE: 1.2989312431187976\n"
     ]
    }
   ],
   "source": [
    "# Complete the steps from quesiton 4 but using RandomizedSearchCV\n",
    "\n",
    "# a) fit a KNN model using the same pipline\n",
    "pipe_knn = Pipeline([\n",
    "  ('impute', SimpleImputer()),\n",
    "  ('poly', PolynomialFeatures(include_bias=False)),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('model', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# b) use grid search with 10-fold cross validation to find the best hyperparameters\n",
    "param_dist = {\n",
    "    'impute__strategy': ['mean', 'median'],\n",
    "    'poly__degree': [1, 2, 3],\n",
    "    'model__n_neighbors': list(range(5, 101, 5)),\n",
    "    'model__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    pipe_knn,\n",
    "    param_distributions=param_dist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "# c) report the best hyperparameters by MSE\n",
    "rs.fit(X_train, y_train)\n",
    "best_params = rs.best_params_\n",
    "best_mse = -rs.best_score_\n",
    "\n",
    "best_params, best_mse\n",
    "\n",
    "# e) use optimized pipline to predit on test data\n",
    "best_model_rs = rs.best_estimator_\n",
    "y_test_pred_knn = best_model_rs.predict(X_test)\n",
    "test_mse_knn = mean_squared_error(y_test, y_test_pred_knn)\n",
    "print(f\"KNN Test MSE: {test_mse_knn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca47617b",
   "metadata": {},
   "source": [
    "6. Advanced Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35361c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   T_degC   Salnty  O2ml_L  Depthm  Bottom_D  Wind_Spd  Dry_T  Wet_T  \\\n",
      "0  16.830  33.8510   5.560      65    1337.0      14.0   16.5   15.5   \n",
      "1   9.262  33.8481   2.729     140    1202.0       5.0   15.0   13.0   \n",
      "2  15.390  33.4260   5.990      30    3871.0      10.0   18.8   17.6   \n",
      "3  14.540  32.9470   5.840      42    4018.0      14.0   16.9   16.1   \n",
      "4   7.410  34.1810   1.000     300    4058.0      21.0   16.3   14.9   \n",
      "\n",
      "                       Wea      Cloud_Typ                  Cloud_Amt  \\\n",
      "0                      NaN            NaN                        NaN   \n",
      "1            Partly Cloudy        Stratus  1/10 or less but not zero   \n",
      "2  Continuous blowing snow        Stratus                      10/10   \n",
      "3  Continuous blowing snow  Stratocumulus                      10/10   \n",
      "4            Partly Cloudy  Stratocumulus               7/10 to 8/10   \n",
      "\n",
      "     Visibility  \n",
      "0           NaN  \n",
      "1  10km to 20km  \n",
      "2  10km to 20km  \n",
      "3   4km to 10km  \n",
      "4   4km to 10km  \n"
     ]
    }
   ],
   "source": [
    "# Adding back in all columns\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfe464dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 11) (1777, 11)\n",
      "Training MSE: 1.535095636242087\n",
      "Test MSE: 1.7543308610948491\n"
     ]
    }
   ],
   "source": [
    "# Create training data and testing data\n",
    "X = df.drop(columns=['T_degC'])\n",
    "y = df['T_degC']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    random_state=307, \n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('num_impute', SimpleImputer(strategy='mean')),\n",
    "    ('poly', PolynomialFeatures(include_bias=False)),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    ('cat_impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('dummy', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_pipeline, numeric_cols),\n",
    "    ('cat', cat_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('dimension', SelectPercentile(f_regression, percentile=50)),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# a) fit pipeline to training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# b) report on training and test MSE\n",
    "y_train_pred = pipe.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aadaa956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lab-03-final-model.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving my model in joblib\n",
    "\n",
    "import joblib\n",
    "joblib.dump(best_model_rs, 'lab-03-final-model.joblib', compress=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
