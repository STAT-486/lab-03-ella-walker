{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c3df2b4",
   "metadata": {},
   "source": [
    "1. Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5d5c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_diabetes, make_regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0280831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   T_degC   Salnty  O2ml_L  Depthm  Bottom_D  Wind_Spd  Dry_T  Wet_T  \\\n",
      "0  16.830  33.8510   5.560      65    1337.0      14.0   16.5   15.5   \n",
      "1   9.262  33.8481   2.729     140    1202.0       5.0   15.0   13.0   \n",
      "2  15.390  33.4260   5.990      30    3871.0      10.0   18.8   17.6   \n",
      "3  14.540  32.9470   5.840      42    4018.0      14.0   16.9   16.1   \n",
      "4   7.410  34.1810   1.000     300    4058.0      21.0   16.3   14.9   \n",
      "\n",
      "                       Wea      Cloud_Typ                  Cloud_Amt  \\\n",
      "0                      NaN            NaN                        NaN   \n",
      "1            Partly Cloudy        Stratus  1/10 or less but not zero   \n",
      "2  Continuous blowing snow        Stratus                      10/10   \n",
      "3  Continuous blowing snow  Stratocumulus                      10/10   \n",
      "4            Partly Cloudy  Stratocumulus               7/10 to 8/10   \n",
      "\n",
      "     Visibility  \n",
      "0           NaN  \n",
      "1  10km to 20km  \n",
      "2  10km to 20km  \n",
      "3   4km to 10km  \n",
      "4   4km to 10km  \n",
      "   T_degC   Salnty  O2ml_L  Depthm  Bottom_D  Wind_Spd  Dry_T  Wet_T\n",
      "0  16.830  33.8510   5.560      65    1337.0      14.0   16.5   15.5\n",
      "1   9.262  33.8481   2.729     140    1202.0       5.0   15.0   13.0\n",
      "2  15.390  33.4260   5.990      30    3871.0      10.0   18.8   17.6\n",
      "3  14.540  32.9470   5.840      42    4018.0      14.0   16.9   16.1\n",
      "4   7.410  34.1810   1.000     300    4058.0      21.0   16.3   14.9\n"
     ]
    }
   ],
   "source": [
    "# Read in the data\n",
    "url = \"https://raw.githubusercontent.com/rhodes-byu/stat-486/main/data/OceanicFisheries/ocean_data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(df.head())\n",
    "\n",
    "# Drop columns that are not needed\n",
    "df = df.drop(columns=['Wea', 'Cloud_Typ', 'Cloud_Amt', 'Visibility'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1c6225e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 7) (1777, 7)\n"
     ]
    }
   ],
   "source": [
    "# Create training data and testing data\n",
    "X = df.drop(columns=['T_degC'])\n",
    "y = df['T_degC']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    random_state=307, \n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89749d31",
   "metadata": {},
   "source": [
    "2. Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16f3f3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 1.5350956362420822\n",
      "Test MSE: 1.7543308610948438\n",
      "Variance of y_test: 14.8162872136925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.11840556515896747)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a) build a pipeline\n",
    "pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# b) fit pipline to training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# c) report on the training MSE\n",
    "y_train_pred = pipe.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "\n",
    "# d) report on the test MSE\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "\n",
    "# e) how does the test MSE compare to the variance of ytest? What does this say about the predictability of your model?\n",
    "y_test_variance = np.var(y_test)\n",
    "print(f\"Variance of y_test: {y_test_variance}\")\n",
    "test_mse / y_test_variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ae643c",
   "metadata": {},
   "source": [
    "3. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f930f7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y-intercept: 11.607865265528682\n"
     ]
    }
   ],
   "source": [
    "# a) use pipeline to order features by magnitude of coefficients\n",
    "betas = pipe.named_steps['model'].coef_\n",
    "feature_names = pipe.named_steps['poly'].get_feature_names_out(X_train.columns)\n",
    "\n",
    "# df of coefficients and feature names\n",
    "coef_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': betas\n",
    "})\n",
    "\n",
    "# order by magnitude of coefficients\n",
    "coef_df['abs_coef'] = coef_df['coefficient'].abs()\n",
    "coef_df_sorted = coef_df.sort_values('abs_coef', ascending=False)\n",
    "\n",
    "coef_df_sorted.head(10)\n",
    "\n",
    "# b top 3 largest positive\n",
    "top_positive = coef_df_sorted.sort_values('coefficient', ascending=False).head(3)\n",
    "top_positive\n",
    "\n",
    "# c top 3 largest negative\n",
    "top_negative = coef_df_sorted.sort_values('coefficient').head(3)\n",
    "top_negative\n",
    "\n",
    "# d y-intercept\n",
    "intercept = pipe.named_steps['model'].intercept_\n",
    "print(f\"y-intercept: {intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c608030",
   "metadata": {},
   "source": [
    "4. Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f329fdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'impute__strategy': 'median',\n",
       "  'model__n_neighbors': 10,\n",
       "  'model__weights': 'distance',\n",
       "  'poly__degree': 3},\n",
       " np.float64(1.1942865460094283))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a) fit a KNN model using the same pipline\n",
    "pipe_knn = Pipeline([\n",
    "  ('impute', SimpleImputer()),\n",
    "  ('poly', PolynomialFeatures(include_bias=False)),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('model', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# b) use grid search with 10-fold cross validation to find the best hyperparameters\n",
    "param_grid = {\n",
    "    'impute__strategy': ['mean', 'median'],\n",
    "    'poly__degree': [1, 2, 3],\n",
    "    'model__n_neighbors': list(range(5, 101, 5)),\n",
    "    'model__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    pipe_knn,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "# c) report the best hyperparameters by MSE\n",
    "gs.fit(X_train, y_train)\n",
    "best_params = gs.best_params_\n",
    "best_mse = -gs.best_score_\n",
    "\n",
    "best_params, best_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b508b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Test MSE: 1.2398437245052845\n"
     ]
    }
   ],
   "source": [
    "# e) use optimized pipline to predit on test data\n",
    "best_model = gs.best_estimator_\n",
    "y_test_pred_knn = best_model.predict(X_test)\n",
    "test_mse_knn = mean_squared_error(y_test, y_test_pred_knn)\n",
    "print(f\"KNN Test MSE: {test_mse_knn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c1a89",
   "metadata": {},
   "source": [
    "5. Randomized Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1aae44c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Test MSE: 1.2989312431187976\n"
     ]
    }
   ],
   "source": [
    "# Complete the steps from quesiton 4 but using RandomizedSearchCV\n",
    "\n",
    "# a) fit a KNN model using the same pipline\n",
    "pipe_knn = Pipeline([\n",
    "  ('impute', SimpleImputer()),\n",
    "  ('poly', PolynomialFeatures(include_bias=False)),\n",
    "  ('scale', StandardScaler()),\n",
    "  ('model', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# b) use grid search with 10-fold cross validation to find the best hyperparameters\n",
    "param_dist = {\n",
    "    'impute__strategy': ['mean', 'median'],\n",
    "    'poly__degree': [1, 2, 3],\n",
    "    'model__n_neighbors': list(range(5, 101, 5)),\n",
    "    'model__weights': ['uniform', 'distance']\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    pipe_knn,\n",
    "    param_distributions=param_dist,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "# c) report the best hyperparameters by MSE\n",
    "rs.fit(X_train, y_train)\n",
    "best_params = rs.best_params_\n",
    "best_mse = -rs.best_score_\n",
    "\n",
    "best_params, best_mse\n",
    "\n",
    "# e) use optimized pipline to predit on test data\n",
    "best_model = rs.best_estimator_\n",
    "y_test_pred_knn = best_model.predict(X_test)\n",
    "test_mse_knn = mean_squared_error(y_test, y_test_pred_knn)\n",
    "print(f\"KNN Test MSE: {test_mse_knn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca47617b",
   "metadata": {},
   "source": [
    "6. Advanced Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35361c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   T_degC   Salnty  O2ml_L  Depthm  Bottom_D  Wind_Spd  Dry_T  Wet_T  \\\n",
      "0  16.830  33.8510   5.560      65    1337.0      14.0   16.5   15.5   \n",
      "1   9.262  33.8481   2.729     140    1202.0       5.0   15.0   13.0   \n",
      "2  15.390  33.4260   5.990      30    3871.0      10.0   18.8   17.6   \n",
      "3  14.540  32.9470   5.840      42    4018.0      14.0   16.9   16.1   \n",
      "4   7.410  34.1810   1.000     300    4058.0      21.0   16.3   14.9   \n",
      "\n",
      "                       Wea      Cloud_Typ                  Cloud_Amt  \\\n",
      "0                      NaN            NaN                        NaN   \n",
      "1            Partly Cloudy        Stratus  1/10 or less but not zero   \n",
      "2  Continuous blowing snow        Stratus                      10/10   \n",
      "3  Continuous blowing snow  Stratocumulus                      10/10   \n",
      "4            Partly Cloudy  Stratocumulus               7/10 to 8/10   \n",
      "\n",
      "     Visibility  \n",
      "0           NaN  \n",
      "1  10km to 20km  \n",
      "2  10km to 20km  \n",
      "3   4km to 10km  \n",
      "4   4km to 10km  \n"
     ]
    }
   ],
   "source": [
    "# Adding back in all columns\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfe464dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 11) (1777, 11)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Names provided are not unique: ['impute', 'poly', 'scale', 'impute', 'dummy', 'dimension', 'model']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# a) fit pipeline to training data\u001b[39;00m\n\u001b[32m     16\u001b[39m pipe = Pipeline([\n\u001b[32m     17\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mimpute\u001b[39m\u001b[33m'\u001b[39m, SimpleImputer(strategy=\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m)),\n\u001b[32m     18\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mpoly\u001b[39m\u001b[33m'\u001b[39m, PolynomialFeatures(degree=\u001b[32m2\u001b[39m, include_bias=\u001b[38;5;28;01mFalse\u001b[39;00m)),\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m, KNeighborsRegressor())\n\u001b[32m     24\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# b) report on training and test MSE\u001b[39;00m\n\u001b[32m     29\u001b[39m y_train_pred = pipe.predict(X_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/pipeline.py:613\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    607\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    608\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m     )\n\u001b[32m    612\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    615\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/pipeline.py:521\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    519\u001b[39m \u001b[38;5;66;03m# shallow copy of steps - this should really be steps_\u001b[39;00m\n\u001b[32m    520\u001b[39m \u001b[38;5;28mself\u001b[39m.steps = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps)\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[38;5;66;03m# Setup the memory\u001b[39;00m\n\u001b[32m    523\u001b[39m memory = check_memory(\u001b[38;5;28mself\u001b[39m.memory)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/pipeline.py:296\u001b[39m, in \u001b[36mPipeline._validate_steps\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    293\u001b[39m names, estimators = \u001b[38;5;28mzip\u001b[39m(*\u001b[38;5;28mself\u001b[39m.steps)\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# validate names\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;66;03m# validate estimators\u001b[39;00m\n\u001b[32m    299\u001b[39m transformers = estimators[:-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/sklearn/utils/metaestimators.py:89\u001b[39m, in \u001b[36m_BaseComposition._validate_names\u001b[39m\u001b[34m(self, names)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_names\u001b[39m(\u001b[38;5;28mself\u001b[39m, names):\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(names)) != \u001b[38;5;28mlen\u001b[39m(names):\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNames provided are not unique: \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mlist\u001b[39m(names)))\n\u001b[32m     90\u001b[39m     invalid_names = \u001b[38;5;28mset\u001b[39m(names).intersection(\u001b[38;5;28mself\u001b[39m.get_params(deep=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m invalid_names:\n",
      "\u001b[31mValueError\u001b[39m: Names provided are not unique: ['impute', 'poly', 'scale', 'impute', 'dummy', 'dimension', 'model']"
     ]
    }
   ],
   "source": [
    "# Create training data and testing data\n",
    "X = df.drop(columns=['T_degC'])\n",
    "y = df['T_degC']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    random_state=307, \n",
    "    test_size=0.25\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# a) fit pipeline to training data\n",
    "pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('impute', SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ('dummy', OneHotEncoder(sparse_output=False, handle_unknown='ignore')),\n",
    "    ('dimension', SelectPercentile(f_regression, percentile=50)),\n",
    "    ('model', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# b) report on training and test MSE\n",
    "y_train_pred = pipe.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Test MSE: {test_mse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
